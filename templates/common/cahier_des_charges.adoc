= Cahier des Charges - Projet Azure Data Factory

:doctype: book

== Introduction

Ce Cahier des Charges définit les spécifications et les exigences du projet Azure Data Factory, qui vise à mettre en place une solution d'ETL (Extraction, Transformation, Loading) pour gérer efficacement les données de l'organisation.

== Objectifs du Projet

Le projet a pour objectif de :

1. Mettre en place une infrastructure Azure Data Factory pour gérer les processus d'ETL.
2. Extraire des données à partir de différentes sources vers Cosmos DB et SQL Server.
3. Appliquer des transformations sur les données selon les besoins métier.
4. Charger les données transformées dans des bases de données pour analyse.
5. Assurer la planification et l'orchestration des pipelines d'ETL.

== Portée du Projet

La portée du projet inclut :

- La configuration de l'Azure Data Factory avec les environnements dev, prp, et prd.
- La création de Linked Services pour Cosmos DB et SQL Server.
- Le développement de pipelines pour l'ETL des données.
- La gestion des dépendances entre les différentes tâches d'ETL.
- La planification et l'orchestration des pipelines.

== Exigences Fonctionnelles

1. **Configuration de l'Azure Data Factory :**
   - Mise en place des environnements dev, prp, et prd.
   - Configuration des Linked Services pour Cosmos DB et SQL Server.

2. **Pipelines d'ETL :**
   - Développement de pipelines pour l'Extraction, la Transformation et le Chargement des données.
   - Gestion des dépendances entre les tâches d'ETL.

3. **Planification et Orchestration :**
   - Planification automatisée des pipelines d'ETL.
   - Orchestration des tâches pour assurer une exécution efficace et sans heurts.

== Exigences Non-Fonctionnelles

1. **Sécurité :**
   - Les accès aux ressources doivent être sécurisés et basés sur les principes du moindre privilège.
   - Utilisation des services managés d'Azure pour garantir la sécurité des données.

2. **Évolutivité :**
   - L'architecture doit être évolutive pour gérer des volumes de données croissants.
   - Les pipelines doivent être conçus pour s'adapter à de nouveaux besoins métier.

3. **Monitoring et Logging :**
   - Mise en place d'une solution de monitoring pour suivre les performances des pipelines.
   - Configuration des logs pour faciliter le dépannage en cas d'erreur.

== Contraintes et Assumptions

1. **Contraintes :**
   - Utilisation d'Azure Data Factory pour la gestion des workflows d'ETL.
   - Utilisation de Terraform comme outil d'IaC pour la gestion de l'infrastructure Azure.

2. **Assumptions :**
   - Les sources de données externes seront accessibles pendant les phases d'ETL.
   - Les environnements Azure (dev, prp, prd) sont correctement configurés.

== Livrables Attendus

1. Code Terraform pour la configuration de l'Azure Data Factory et des ressources associées.
2. Définition des pipelines d'ETL dans Azure Data Factory.
3. Documentation détaillée des processus d'ETL et de la configuration de l'infrastructure.
4. Plan de déploiement et procédures d'installation.

== Calendrier et Phases du Projet

Le projet sera réalisé en plusieurs phases :

1. **Phase de Configuration Initiale (Semaines 1-2) :**
   - Configuration des environnements Azure.
   - Mise en place des Linked Services.

2. **Phase de Développement des Pipelines (Semaines 3-6) :**
   - Développement des pipelines d'Extraction, Transformation et Chargement.
   - Gestion des dépendances entre les tâches.

3. **Phase de Planification et Orchestration (Semaines 7-8) :**
   - Planification automatisée des pipelines.
   - Orchestration des tâches pour garantir une exécution sans heurts.

4. **Phase de Test et Validation (Semaines 9-10) :**
   - Tests unitaires des pipelines.
   - Validation des performances et de la scalabilité.

5. **Phase de Documentation et Livraison (Semaines 11-12) :**
   - Documentation complète des processus et de la configuration.
   - Préparation des livrables finaux.

== Approbation du Cahier des Charges

